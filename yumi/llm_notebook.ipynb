{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yumi Chatbot R&D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.globals import set_debug\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnablePassthrough,\n",
    "    RunnableSerializable,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from prompts.gen_prompts import GENERAL_PROMPT, RAG_PROMPT\n",
    "from rag_pinecone import basic_retriever\n",
    "\n",
    "set_debug(True)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "mistralai_api_key = os.getenv(\"MISTRALAI_API_KEY\")\n",
    "\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    google_api_key=f\"{google_api_key}\",\n",
    "    model=\"gemini-pro\",\n",
    ")  # Type: Ignore\n",
    "mistral_llm = ChatMistralAI(model=\"mistral-large-latest\")\n",
    "openai_llm = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_memory(\n",
    "    stored_session: InMemoryChatMessageHistory,\n",
    ") -> InMemoryChatMessageHistory:\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"The mesages above are from an AI/Human chat session. You need to distill the above chat messages into a single summary message. Include as many specific details as you can. But be sure that it is done in a way that is concise and easy to understand as it will be used to summarize the chat history and used as reference later by the AI\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    summarization_chain = (summarization_prompt | gemini_llm).with_config(\n",
    "        config={\"run_name\": \"sumarize_memory\"}\n",
    "    )\n",
    "    summary_message = summarization_chain.invoke({\"history\": stored_session.messages})\n",
    "    stored_session.clear()\n",
    "    stored_session.add_message(summary_message)\n",
    "    return stored_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_memory_token_size(messages: BaseChatMessageHistory) -> bool:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    count = []\n",
    "    for message in messages:\n",
    "        token_count = len(encoding.encode(message.content))\n",
    "        count.append(token_count)\n",
    "    total_tokens = sum(count)\n",
    "    if total_tokens > 100:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in memory_store:\n",
    "        memory_store[session_id] = ChatMessageHistory()\n",
    "        return memory_store[session_id]\n",
    "    stored_session: InMemoryChatMessageHistory = memory_store[session_id]\n",
    "    if len(stored_session.messages) > 6:\n",
    "        if check_memory_token_size(stored_session.messages):\n",
    "            return summarize_memory(stored_session)\n",
    "    return stored_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Runnables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baisc_conversation(\n",
    "    query: Dict[str, Any], config: Dict[str, Dict[str, Any]] = None\n",
    ") -> RunnableWithMessageHistory:\n",
    "    basic_convo = GENERAL_PROMPT | openai_llm | StrOutputParser()\n",
    "    with_message_history = RunnableWithMessageHistory(\n",
    "        basic_convo,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"query\",\n",
    "        history_messages_key=\"history\",\n",
    "    )\n",
    "    return with_message_history.invoke(query, config)\n",
    "\n",
    "\n",
    "baisc_conversation(\n",
    "    {\n",
    "        \"query\": \"Can you kindly list all the cities we spoke about?\",\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"def234\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(memory_store[\"def234\"].messages))\n",
    "print(check_memory_token_size(memory_store[\"def234\"].messages))\n",
    "print(memory_store[\"def234\"].messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = await basic_retriever(query=\"Who is Alis?\")\n",
    "# print(len(docs))\n",
    "# print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_runnable = RunnableLambda(basic_retriever)\n",
    "basic_convo = RAG_PROMPT | openai_llm | StrOutputParser()\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    basic_convo,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "query = \"Who is Alis?\"\n",
    "chain = (\n",
    "    {\"context\": retriever_runnable, \"query\": RunnablePassthrough()}\n",
    "    | with_message_history\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "await chain.ainvoke(\"Who is Alis?\", config={\"configurable\": {\"session_id\": \"def234\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def basic_rag_conversation(\n",
    "    query: str, config: Dict[str, Dict[str, Any]]\n",
    ") -> RunnableSerializable:\n",
    "    basic_convo = RAG_PROMPT | openai_llm | StrOutputParser()\n",
    "    with_message_history = RunnableWithMessageHistory(\n",
    "        basic_convo,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"query\",\n",
    "        history_messages_key=\"history\",\n",
    "    )\n",
    "    retriever_runnable = RunnableLambda(basic_retriever)\n",
    "    chain = (\n",
    "        {\"context\": retriever_runnable, \"query\": RunnablePassthrough()}\n",
    "        | with_message_history\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    response = await chain.ainvoke(query, config)\n",
    "    return response\n",
    "\n",
    "\n",
    "response = basic_rag_conversation(\n",
    "    \"Who is Alis?\", {\"configurable\": {\"session_id\": \"def234\"}}\n",
    ")\n",
    "\n",
    "print(await response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import chat_agent_executor\n",
    "\n",
    "from llm import baisc_conversation, basic_rag_conversation, get_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    basic_rag_conversation,\n",
    "    \"Basic_Rag_Retriever\",\n",
    "    \"A tool used for basic retrival of PDF documents from a Vectorstore in Pincone.\",\n",
    ")\n",
    "weather_tool = create_retriever_tool(\n",
    "    get_weather,\n",
    "    \"Weather_Tool\",\n",
    "    \"A tool used to get the weather for a specific location.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool, weather_tool]\n",
    "\n",
    "# model_with_tools = openai_llm.bind_tools(tools)\n",
    "# response = model_with_tools.invoke(\n",
    "#     [HumanMessage(content=\"According to the document, who is Alis Landale?\")]\n",
    "# )\n",
    "# response.content\n",
    "agent_executor = chat_agent_executor.create_tool_calling_executor(openai_llm, tools)\n",
    "response = await agent_executor.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the weather in New York?\")]},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")\n",
    "\n",
    "response[\"messages\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yumi-py3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
