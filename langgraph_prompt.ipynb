{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import uuid\n",
    "from textwrap import dedent\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Annotated\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AnyMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, MessageGraph, add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "logger = logging.getLogger(__name__)\n",
    "console = logging.StreamHandler()\n",
    "logger.addHandler(console)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = dedent(\n",
    "    \"\"\"\n",
    "Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool\n",
    "to generate the prompt template and return it to the user.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_messages_info(\n",
    "    messages: Annotated[List[AnyMessage], \"messages\"]\n",
    ") -> List[AnyMessage]:\n",
    "    logger.debug(\"get_messages_info: %s\", messages)\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableLambda(get_messages_info)\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12064ab50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x121466010>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'PromptInstructions', 'description': 'Instructions on how to prompt the LLM.', 'parameters': {'type': 'object', 'properties': {'objective': {'type': 'string'}, 'variables': {'type': 'array', 'items': {'type': 'string'}}, 'constraints': {'type': 'array', 'items': {'type': 'string'}}, 'requirements': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['objective', 'variables', 'constraints', 'requirements']}}}]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "chain = get_messages_info | llm_with_tool\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for determining if tool was called\n",
    "def _is_tool_call(msg):\n",
    "    logger.debug(\"Checking if tool was called: %s\", msg)\n",
    "    return hasattr(msg, \"additional_kwargs\") and \"tool_calls\" in msg.additional_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New system prompt\n",
    "prompt_system = dedent(\n",
    "    \"\"\"\\\n",
    "Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Function to get the messages for the prompt\n",
    "# Will only get messages AFTER the tool call\n",
    "def get_prompt_messages(messages):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if _is_tool_call(m):\n",
    "            tool_call = m.additional_kwargs[\"tool_calls\"][0][\"function\"][\"arguments\"]\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_gen_chain = get_prompt_messages | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableLambda(get_prompt_messages) last=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12064ab50>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x121466010>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "print(prompt_gen_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(messages):\n",
    "    if _is_tool_call(messages[-1]):\n",
    "        return \"prompt\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    for m in messages:\n",
    "        if _is_tool_call(m):\n",
    "            return \"prompt\"\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', 'aget', 'aget_tuple', 'alist', 'aput', 'asearch', 'config_specs', 'conn', 'cursor', 'from_conn_string', 'get', 'get_tuple', 'is_setup', 'list', 'lock', 'put', 'search', 'serde', 'setup']\n"
     ]
    }
   ],
   "source": [
    "memory: SqliteSaver = SqliteSaver.from_conn_string(\":memory:\")\n",
    "nodes = {k: k for k in [\"info\", \"prompt\", END]}\n",
    "workflow = MessageGraph()\n",
    "print(dir(memory))\n",
    "workflow.add_node(\"info\", chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "workflow.add_conditional_edges(\"info\", get_state, nodes)\n",
    "workflow.add_conditional_edges(\"prompt\", get_state, nodes)\n",
    "workflow.set_entry_point(\"info\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_messages_info: [HumanMessage(content='Hello', id='4465a8b5-70b5-4f2f-881d-599c7478f888')]\n",
      "get_messages_info: [HumanMessage(content='Hello', id='4465a8b5-70b5-4f2f-881d-599c7478f888')]\n",
      "Checking if tool was called: content='Hello! How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'\n",
      "Checking if tool was called: content='Hello! How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Hello! How can I assist you today?' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_messages_info: [HumanMessage(content='Hello', id='4465a8b5-70b5-4f2f-881d-599c7478f888'), AIMessage(content='Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}}, id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'), HumanMessage(content='I would like a prompt that chooses a color and the varialble is the color', id='fcb89bbb-d53c-41be-8ba9-f654e97a4a9f')]\n",
      "get_messages_info: [HumanMessage(content='Hello', id='4465a8b5-70b5-4f2f-881d-599c7478f888'), AIMessage(content='Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}}, id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'), HumanMessage(content='I would like a prompt that chooses a color and the varialble is the color', id='fcb89bbb-d53c-41be-8ba9-f654e97a4a9f')]\n",
      "Checking if tool was called: content='Great! Could you please provide more information about the objective of the prompt, any constraints for what the output should NOT do, and any requirements that the output MUST adhere to? This will help me create a prompt template that meets your needs.' response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 215, 'total_tokens': 264}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1fc84770-166a-4295-b595-e14b33604dac-0'\n",
      "Checking if tool was called: content='Great! Could you please provide more information about the objective of the prompt, any constraints for what the output should NOT do, and any requirements that the output MUST adhere to? This will help me create a prompt template that meets your needs.' response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 215, 'total_tokens': 264}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1fc84770-166a-4295-b595-e14b33604dac-0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Great! Could you please provide more information about the objective of the prompt, any constraints for what the output should NOT do, and any requirements that the output MUST adhere to? This will help me create a prompt template that meets your needs.' response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 215, 'total_tokens': 264}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1fc84770-166a-4295-b595-e14b33604dac-0'\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_messages_info: [HumanMessage(content='Hello', id='4465a8b5-70b5-4f2f-881d-599c7478f888'), AIMessage(content='Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}}, id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'), HumanMessage(content='I would like a prompt that chooses a color and the varialble is the color', id='fcb89bbb-d53c-41be-8ba9-f654e97a4a9f'), AIMessage(content='Great! Could you please provide more information about the objective of the prompt, any constraints for what the output should NOT do, and any requirements that the output MUST adhere to? This will help me create a prompt template that meets your needs.', response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 49, 'prompt_tokens': 215, 'total_tokens': 264}}, id='run-1fc84770-166a-4295-b595-e14b33604dac-0'), HumanMessage(content='It will be a prompt to choice you favorite color and then come up with the opposisit color. The color is the varialble there is no constraints.', id='447fad0b-4f39-4342-8e96-2e696ba5bd90')]\n",
      "get_messages_info: [HumanMessage(content='Hello', id='4465a8b5-70b5-4f2f-881d-599c7478f888'), AIMessage(content='Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}}, id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'), HumanMessage(content='I would like a prompt that chooses a color and the varialble is the color', id='fcb89bbb-d53c-41be-8ba9-f654e97a4a9f'), AIMessage(content='Great! Could you please provide more information about the objective of the prompt, any constraints for what the output should NOT do, and any requirements that the output MUST adhere to? This will help me create a prompt template that meets your needs.', response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 49, 'prompt_tokens': 215, 'total_tokens': 264}}, id='run-1fc84770-166a-4295-b595-e14b33604dac-0'), HumanMessage(content='It will be a prompt to choice you favorite color and then come up with the opposisit color. The color is the varialble there is no constraints.', id='447fad0b-4f39-4342-8e96-2e696ba5bd90')]\n",
      "Checking if tool was called: content='' additional_kwargs={'tool_calls': [{'id': 'call_xtuiZSporCUumEJ1U27CJRA9', 'function': {'arguments': '{\"objective\":\"Choose your favorite color and determine its opposite color.\",\"variables\":[\"color\"],\"constraints\":[],\"requirements\":[]}', 'name': 'PromptInstructions'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 304, 'total_tokens': 337}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-335e3a91-c47f-445b-82e5-55f2ef372468-0' tool_calls=[{'name': 'PromptInstructions', 'args': {'objective': 'Choose your favorite color and determine its opposite color.', 'variables': ['color'], 'constraints': [], 'requirements': []}, 'id': 'call_xtuiZSporCUumEJ1U27CJRA9'}]\n",
      "Checking if tool was called: content='' additional_kwargs={'tool_calls': [{'id': 'call_xtuiZSporCUumEJ1U27CJRA9', 'function': {'arguments': '{\"objective\":\"Choose your favorite color and determine its opposite color.\",\"variables\":[\"color\"],\"constraints\":[],\"requirements\":[]}', 'name': 'PromptInstructions'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 304, 'total_tokens': 337}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-335e3a91-c47f-445b-82e5-55f2ef372468-0' tool_calls=[{'name': 'PromptInstructions', 'args': {'objective': 'Choose your favorite color and determine its opposite color.', 'variables': ['color'], 'constraints': [], 'requirements': []}, 'id': 'call_xtuiZSporCUumEJ1U27CJRA9'}]\n",
      "Checking if tool was called: content='Hello' id='4465a8b5-70b5-4f2f-881d-599c7478f888'\n",
      "Checking if tool was called: content='Hello' id='4465a8b5-70b5-4f2f-881d-599c7478f888'\n",
      "Checking if tool was called: content='Hello! How can I assist you today?' response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}} id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'\n",
      "Checking if tool was called: content='Hello! How can I assist you today?' response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 10, 'prompt_tokens': 181, 'total_tokens': 191}} id='run-9dd2e7bf-3a99-4b7f-84d7-bccdb08fe74a-0'\n",
      "Checking if tool was called: content='I would like a prompt that chooses a color and the varialble is the color' id='fcb89bbb-d53c-41be-8ba9-f654e97a4a9f'\n",
      "Checking if tool was called: content='I would like a prompt that chooses a color and the varialble is the color' id='fcb89bbb-d53c-41be-8ba9-f654e97a4a9f'\n",
      "Checking if tool was called: content='Great! Could you please provide more information about the objective of the prompt, any constraints for what the output should NOT do, and any requirements that the output MUST adhere to? This will help me create a prompt template that meets your needs.' response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 49, 'prompt_tokens': 215, 'total_tokens': 264}} id='run-1fc84770-166a-4295-b595-e14b33604dac-0'\n",
      "Checking if tool was called: content='Great! Could you please provide more information about the objective of the prompt, any constraints for what the output should NOT do, and any requirements that the output MUST adhere to? This will help me create a prompt template that meets your needs.' response_metadata={'finish_reason': 'stop', 'logprobs': None, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'token_usage': {'completion_tokens': 49, 'prompt_tokens': 215, 'total_tokens': 264}} id='run-1fc84770-166a-4295-b595-e14b33604dac-0'\n",
      "Checking if tool was called: content='It will be a prompt to choice you favorite color and then come up with the opposisit color. The color is the varialble there is no constraints.' id='447fad0b-4f39-4342-8e96-2e696ba5bd90'\n",
      "Checking if tool was called: content='It will be a prompt to choice you favorite color and then come up with the opposisit color. The color is the varialble there is no constraints.' id='447fad0b-4f39-4342-8e96-2e696ba5bd90'\n",
      "Checking if tool was called: content='' additional_kwargs={'tool_calls': [{'id': 'call_xtuiZSporCUumEJ1U27CJRA9', 'function': {'arguments': '{\"objective\":\"Choose your favorite color and determine its opposite color.\",\"variables\":[\"color\"],\"constraints\":[],\"requirements\":[]}', 'name': 'PromptInstructions'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 304, 'total_tokens': 337}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-335e3a91-c47f-445b-82e5-55f2ef372468-0' tool_calls=[{'name': 'PromptInstructions', 'args': {'objective': 'Choose your favorite color and determine its opposite color.', 'variables': ['color'], 'constraints': [], 'requirements': []}, 'id': 'call_xtuiZSporCUumEJ1U27CJRA9'}]\n",
      "Checking if tool was called: content='' additional_kwargs={'tool_calls': [{'id': 'call_xtuiZSporCUumEJ1U27CJRA9', 'function': {'arguments': '{\"objective\":\"Choose your favorite color and determine its opposite color.\",\"variables\":[\"color\"],\"constraints\":[],\"requirements\":[]}', 'name': 'PromptInstructions'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 304, 'total_tokens': 337}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-335e3a91-c47f-445b-82e5-55f2ef372468-0' tool_calls=[{'name': 'PromptInstructions', 'args': {'objective': 'Choose your favorite color and determine its opposite color.', 'variables': ['color'], 'constraints': [], 'requirements': []}, 'id': 'call_xtuiZSporCUumEJ1U27CJRA9'}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_xtuiZSporCUumEJ1U27CJRA9', 'function': {'arguments': '{\"objective\":\"Choose your favorite color and determine its opposite color.\",\"variables\":[\"color\"],\"constraints\":[],\"requirements\":[]}', 'name': 'PromptInstructions'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 304, 'total_tokens': 337}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None} id='run-335e3a91-c47f-445b-82e5-55f2ef372468-0' tool_calls=[{'name': 'PromptInstructions', 'args': {'objective': 'Choose your favorite color and determine its opposite color.', 'variables': ['color'], 'constraints': [], 'requirements': []}, 'id': 'call_xtuiZSporCUumEJ1U27CJRA9'}]\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking if tool was called: content='What is your favorite color? Determine its opposite color.' response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 43, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1fa21b51-a1ce-4f45-92e5-1fa99fd31aea-0'\n",
      "Checking if tool was called: content='What is your favorite color? Determine its opposite color.' response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 43, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1fa21b51-a1ce-4f45-92e5-1fa99fd31aea-0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'prompt':\n",
      "---\n",
      "content='What is your favorite color? Determine its opposite color.' response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 43, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1fa21b51-a1ce-4f45-92e5-1fa99fd31aea-0'\n",
      "\n",
      "---\n",
      "\n",
      "AI: Byebye\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    user = input(\"User (q/Q to quit): \")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    for output in graph.stream([HumanMessage(content=user)], config=config):\n",
    "        if \"__end__\" in output:\n",
    "            continue\n",
    "        # stream() yields dictionaries with output keyed by node name\n",
    "        for key, value in output.items():\n",
    "            print(f\"Output from node '{key}':\")\n",
    "            print(\"---\")\n",
    "            print(value)\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yumi-3kP-LFxI-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
